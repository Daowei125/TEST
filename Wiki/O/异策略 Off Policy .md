# 异策略 Off Policy 

**异策略**指生成新样本的策略跟网络更新参数时使用的策略不同。典型的异策略如 Q-learning 算法。

### 异策略思想

异策略学习指想要学习的是一个策略，而用于采样的是另一个策略。

先产生某概率分布下的大量行为数据，意在探索。从这些偏离（ off ）最优策略的数据中寻求目标策略。

当然这么做是需要满足数学条件的：假設 π 是目标策略, µ 是行为策略，那么从 µ 学到 π 的条件是：π ( a | s ) > 0 必然有 µ ( a | s ) > 0 成立。

### Q-learning 算法

Q-learning 通过感知奖励和惩罚，学习如何选择下一步动作，Q-Learning 算法中的 “Q” 代表着策略 π 的质量函数 Quality function ，该函数能在观察状态 s 确定动作 a 后，把每个状态动作对 ( s , a ) 与总期望的折扣未来奖励进行映射。

Q-Learning 算法属于 model-free 型，这意味着它不会对 MDP 动态知识进行建模，而是直接估计每个状态下每个动作的 Q 值 。然后，通过在每个状态下选择具有最高Q值的动作，来选择相应的策略。

如果计算机不断地访问所有状态动作对，则 Q-Learning 算法会收敛到最优 Q 函数。

### 异策略的优势 


1）可以从人类给出的示教样本或其他智能体给出的引导样本中学习；

2）可以重用由旧策略生成的经验；

3）可以在使用一个探索性策略的同时学习一个确定性策略；

4）可以用一个策略进行采样，然后同时学习多个策略。

##### 相关词：同策略，策略函数。

### 参考来源

【1】  https://blog.csdn.net/u013745804/article/details/78238678

【2】  https://blog.csdn.net/weixin_37895339/article/details/74937023

【3】  http://www.sohu.com/a/232174380_610300
