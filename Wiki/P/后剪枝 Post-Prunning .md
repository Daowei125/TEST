# 后剪枝 Post-Prunning 

**后剪枝**是剪枝算法的重要一类。是指在决策树生成后进行的剪枝操作。    


后剪枝方法首先要构造完整的决策树，允许树过度拟合训练数据，然后对置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。

后剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。后剪枝是目前最普遍的做法。

### 后剪枝的方法：

基于已有的树切分测试数据： 

1）如果存在任一子集是一棵树，则在该子集递归剪枝过程； 
2) 计算不合并的误差；
3) 如果合并会降低误差的话，就将叶节点合并。

### 后剪枝算法列举：

1）错误率降低剪枝 REP ，（ Reduced-Error Pruning ）；

2）悲观剪枝代价 EBP ， ( Error-Based Pruning )； 

3）复杂度剪枝 CCP ，（ Cost-Complexity Pruning ）；  

4）基于错误的剪枝 PEP ，（ Pesimistic-Error Pruning ）。



### 前剪枝和后剪枝的对比：

前阈值的设定很敏感，一点点的变动，会引起整颗树非常大的变动，不好设定。前剪枝生成比后剪枝简洁的树。一般用后剪得到的结果比较好。

后剪枝可以保留了更多的分支，相比于预剪枝欠拟合风险更小，但是后剪枝是根据已经训练好的决策树自底向上逐层扫面判定，因此会导致训练的时间和开销比预剪枝较大。

相比于先剪枝，后剪枝方法更常用，是因为在先剪枝方法中精确地估计何时停止树增长很困难。

##### 父级词：剪枝  
##### 关联词：预剪枝

### 参考来源：

【1】  https://www.deeplearn.me/645.html

【2】  http://isilic.iteye.com/blog/1846726
