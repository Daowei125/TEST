# 剪枝 Pruning


**剪枝**是让决策树停止分支的一种方法。是为了解决决策树出现过拟合问题时的处理手段。

决策树学习过程中，为了尽可能正确分类训练样本，会不断地生成结点，有时会造成决策树分支过多，此时就需要对其进行剪枝操作，对决策树进行简化。   

### 剪枝的意义

决策树算法中的问题之一是决定树的最佳大小。太大的树可能会出现过拟合，并且很难推广到新样本。小树可能无法捕获有关样本空间的重要结构信息。

但是，很难判断树算法何时应该停止，因为无法判断添加单个额外节点是否会显着减少错误。一种常见的策略是增长树，直到每个节点包含少量实例，然后使用剪枝来删除不提供其他信息的节点。

### 剪枝的思路及做法

剪枝的准则是如何确定决策树的规模：

1：使用训练集合( Training Set ）和验证集合( Validation Set )，来评估剪枝方法在修剪结点上的效用。

2：使用所有的训练集合进行训练，但是用统计测试来估计修剪特定结点是否会改善训练集合外的数据的评估性能，如使用 Chi-Square（ Quinlan，1986 ）测试来进一步扩展结点是否能改善整个分类数据的性能，还是仅仅改善了当前训练集合数据上的性能。

3：使用明确的标准来衡量训练样例和决策树的复杂度，当编码长度最小时，停止树增长，如 MDL ( Minimum Description Length )准则。

剪枝的具体操作为:

从决策树上减掉一些子树或叶结点，然后将根结点或父节点作为叶节点。

### 剪枝的分类

剪枝通常分为两类：预剪枝（ Pre-Pruning ）和后剪枝（ Post-Pruning ）（详见子词条）

##### 父词汇：决策树。
##### 子词汇：预剪枝，后剪枝。
    
### 参考来源：

[1]  https://en.wikipedia.org/wiki/Pruning_(decision_trees)

[2]  https://www.displayr.com/machine-learning-pruning-decision-trees/