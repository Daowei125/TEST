# 梯度消失问题 Vanishing Gradient Problem

**梯度消失问题**是一种在使用梯度下降法和反向传播训练人工神经网络时遇到的难题。在这类训练方法的每个迭代中，神经网络权重的更新值与误差函数梯度成比例，然而在某些情况下，梯度值会几乎消失，使得权重无法得到有效更新，甚至神经网络可能完全无法继续训练。

举例来说，传统的激活函数，如双曲正切函数的梯度值在 ( 0 , 1 )范围内，而反向传播通过链式法则来计算梯度。 这种做法计算前一层的梯度时，相当于将 n 个这样小的数字相乘，这就使梯度（误差信号）随 n 呈指数下降，导致前面的层训练非常缓慢。


### 参考来源

【1】  https://zh.wikipedia.org/wiki/梯度消失问题