# 再缩放 Rescaling

**再缩放** 也称为再平衡，是处理类别不平衡问题时的一种基本策略。再缩放，也是代价敏感学习的基础。


![](再缩放.jpeg)

再缩放需要假设“训练集是真实样本总体的无偏采样”，这个假设往往并不成立，现在技术上有三类做法：

1.“欠采样”( undersampling )，去掉一些反例使正、反例数目接近，这样丢弃了很多反例使得训练集大大减少，可能丢失掉一些重要的信息；

2.“过采样”( oversampling )，增加一些正例使得数目接近，增大了训练集，不能简单的对样本进行重复采样，否则很容易过拟合，好的办法是插值产生额外正例；

3.“阈值移动”( threshold-moving )，在训练的过程中使用全部的数据进行训练，在预测的时候将上面的再放缩技巧引入得到结果。


### 参考来源

【1】 《机器学习》，周志华，清华大学出版社