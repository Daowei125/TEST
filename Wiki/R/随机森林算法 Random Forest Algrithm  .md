# 随机森林算法 Random Forest Algrithm 

**随机森林**是一种多功能的机器学习算法，能够执行回归和分类的任务。同时，它也是一种数据降维手段，用于处理缺失值、异常值以及其他数据探索中的重要步骤。另外，它还是集成学习中的重要方法，会用在将几个低效模型整合为一个高效模型。


### 随机森林的描述

随机森林用有放回抽样（ Bootstrap 抽样）构成出的样本集训练多棵决策树，训练决策树的每个节点时只使用了随机抽样的部分特征。

当在基于某些属性对一个新的对象进行分类判别时，随机森林中的每一棵树都会给出自己的分类选择，并由此进行“投票”，森林整体的输出结果将会是票数最多的分类选项；而在回归问题中，随机森林的输出将会是所有决策树输出的平均值。

随机森林算法中，“随机”是其核心灵魂，“森林”只是一种简单的组合方式而已。随机森林在构建每颗树的时候，为了保证各树之间的独立性，通常会采用两到三层的随机性。



### 随机森林的构建过程

由于随机森林是集成学习中的 Bagging 算法，所以过程和 Baging 类似：

1. 从原始训练集中使用 Bootstraping 方法随机有放回采样选出 m 个样本，共进行 n 次采样，生成 n 个训练集。
2. 对于 n 个训练集，我们分别训练 n 个决策树模型。
3. 对于单个决策树模型，假设训练样本特征的个数为 n<sub>0</sub>，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂。
4. 每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝。
5. 将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果。


### 随机森林的特点：

优点：

- 具有极高的准确率。

- 不容易过拟合。
- 很好的抗噪声能力。
- 能处理很高维度的数据，并且不用做特征选择。 
- 既能处理离散型数据，也能处理连续型数据。
- 数据集无需规范化。
- 训练速度快，可以得到变量重要性排序。 
- 容易实现并行化。

缺点：

- 参数较复杂。

- 训练时需要的空间和时间会较大。 
- 随机森林模型还有许多不好解释的地方。


##### 父级词：Bagging算法


### 参考来源：

【1】  https://blog.csdn.net/qq547276542/article/details/78304454

【2】  https://blog.csdn.net/lishuandao/article/details/52555103

【3】  https://en.wikipedia.org/wiki/Random_forest
http://dataunion.org/23602.html